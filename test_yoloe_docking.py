"""
YOLOE ê¸°ë°˜ KABOAT ë„í‚¹ ì‹œìŠ¤í…œ
Real-Time Seeing Anything - ì‹¤ì‹œê°„ ì˜¤í”ˆì›”ë“œ ê°ì²´ íƒì§€

YOLOE íŠ¹ì§• (ICCV 2025):
- YOLO-Worldv2 ëŒ€ë¹„ +3.5 AP, 1.4ë°° ë¹ ë¥¸ ì¶”ë¡ 
- YOLOv8-L ëŒ€ë¹„ í•™ìŠµ ì‹œê°„ 1/4
- 3ê°€ì§€ í”„ë¡¬í”„íŠ¸ ë°©ì‹:
  1ï¸âƒ£ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: í´ë˜ìŠ¤ ì´ë¦„ìœ¼ë¡œ íƒì§€
  2ï¸âƒ£ ë¹„ì£¼ì–¼ í”„ë¡¬í”„íŠ¸: ì˜ˆì‹œ ì´ë¯¸ì§€ë¡œ ìœ ì‚¬ ê°ì²´ íƒì§€
  3ï¸âƒ£ í”„ë¡¬í”„íŠ¸ ì—†ìŒ: ëª¨ë“  ê°ì²´ ìë™ íƒì§€
- Zero-shot ì„±ëŠ¥ SOTA

KABOAT 2025 íƒì§€ ëŒ€ìƒ:
- ë„í‚¹: ì‚¼ê°í˜•/ì›í˜•/ë„¤ëª¨ Ã— ë¹¨ê°•/ì´ˆë¡/íŒŒë‘/ì£¼í™©/ë…¸ë‘
- ì¥ì• ë¬¼: ì£¼í™©ìƒ‰ ë¶€í‘œ/ì›ë¿”
- ìœ„ì¹˜ìœ ì§€: ë…¸ë€ìƒ‰ ë¶€í‘œ
- íƒìƒ‰: ë¹¨ê°•/ì´ˆë¡/íŒŒë‘ ë¶€í‘œ
- í•­ë¡œì¶”ì¢…: ë¹¨ê°•/ì´ˆë¡ ê²Œì´íŠ¸ ë¶€í‘œ
"""

import os
import torch
from PIL import Image, ImageDraw
import json
from datetime import datetime
import time
import cv2
import numpy as np

try:
    from ultralytics import YOLO
    YOLOE_AVAILABLE = True
except ImportError:
    YOLOE_AVAILABLE = False
    print("ê²½ê³ : ultralytics íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜ ë°©ë²•: pip install ultralytics")


class YOLOEDockingDetector:
    def __init__(self, model_name="yolov8l.pt", confidence_threshold=0.1, device="cuda", use_text_prompt=True):
        """YOLOE ê¸°ë°˜ íƒì§€ê¸° ì´ˆê¸°í™”

        Args:
            model_name: YOLOE ëª¨ë¸ ì´ë¦„ (yolov8l-world.pt ê¶Œì¥)
            confidence_threshold: íƒì§€ ì‹ ë¢°ë„ ì„ê³„ê°’
            device: ë””ë°”ì´ìŠ¤ (cuda/cpu)
            use_text_prompt: í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© ì—¬ë¶€
        """
        if not YOLOE_AVAILABLE:
            raise ImportError("ultralytics íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install ultralytics")

        print(f"=" * 80)
        print(f"YOLOE KABOAT ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
        print(f"=" * 80)
        print(f"ëª¨ë¸: {model_name}")
        print(f"ë””ë°”ì´ìŠ¤: {device}")
        print(f"ì„ê³„ê°’: {confidence_threshold}")
        print(f"í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: {use_text_prompt}")

        self.device = device if torch.cuda.is_available() else "cpu"
        self.threshold = confidence_threshold
        self.use_text_prompt = use_text_prompt

        # YOLOE ëª¨ë¸ ë¡œë“œ
        print(f"\nYOLOE ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.model = YOLO(model_name)

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if self.device == "cuda":
            self.model.to("cuda")

        # KABOAT 2025 í´ë˜ìŠ¤ ì •ì˜ (í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸)
        self.kaboat_classes = self._define_kaboat_classes()

        if self.use_text_prompt and hasattr(self.model, 'set_classes'):
            print(f"\nğŸ“ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ëª¨ë“œ: {len(self.kaboat_classes)}ê°œ í´ë˜ìŠ¤ ì„¤ì •")
            self.model.set_classes(self.kaboat_classes)
        else:
            print(f"\nğŸ” ìë™ íƒì§€ ëª¨ë“œ: í”„ë¡¬í”„íŠ¸ ì—†ì´ ëª¨ë“  ê°ì²´ íƒì§€")

        print("âœ“ YOLOE ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {allocated:.2f}GB (ì˜ˆì•½: {reserved:.2f}GB)")

        print("=" * 80)

    def _define_kaboat_classes(self):
        """KABOAT 2025 ëŒ€íšŒ ê·œì •ì— ë”°ë¥¸ íƒì§€ í´ë˜ìŠ¤ ì •ì˜

        Returns:
            list: íƒì§€í•  ê°ì²´ í´ë˜ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        classes = []

        # 1. ë„í‚¹ ì„ë¬´ í‘œì‹ (í˜•ìƒ Ã— ìƒ‰ìƒ)
        docking_shapes = ["triangle", "circle", "square", "triangular marker", "circular marker", "square marker"]
        docking_colors = ["red", "green", "blue", "orange", "yellow"]

        # ë‹¨ì¼ ìƒ‰ìƒ í˜•ìƒ
        classes.extend([f"{color} {shape}" for color in docking_colors for shape in ["triangle", "circle", "square"]])

        # 2. ë¶€í‘œ (Buoy) - ëª¨ë“  ì„ë¬´ì—ì„œ ì‚¬ìš©
        buoy_types = [
            # ì¥ì• ë¬¼ íšŒí”¼
            "orange buoy", "orange cone", "orange marker", "orange obstacle",

            # ìœ„ì¹˜ìœ ì§€
            "yellow buoy", "yellow marker", "yellow waypoint",

            # íƒìƒ‰ ë° í•­ë¡œì¶”ì¢…
            "red buoy", "green buoy", "blue buoy",

            # ê²Œì´íŠ¸ ë¶€í‘œ
            "navigation buoy", "gate buoy", "marker buoy",

            # ì¼ë°˜ ë¶€í‘œ
            "buoy", "floating marker", "marine marker"
        ]
        classes.extend(buoy_types)

        # 3. ë„í‚¹ ìŠ¤í…Œì´ì…˜
        docking_station = [
            "docking station", "dock", "landing pad",
            "docking platform", "berthing area"
        ]
        classes.extend(docking_station)

        # 4. ë°° ë° ì¥ì• ë¬¼
        obstacles = [
            "boat", "ship", "vessel", "other boat",
            "obstacle", "barrier"
        ]
        classes.extend(obstacles)

        # ì¤‘ë³µ ì œê±°
        return list(set(classes))

    def detect_markers(self, image_path, text_queries=None):
        """í‘œì‹ íƒì§€ (3ê°€ì§€ í”„ë¡¬í”„íŠ¸ ë°©ì‹ ì§€ì›)

        Args:
            image_path: ì´ë¯¸ì§€ ê²½ë¡œ
            text_queries: íƒì§€í•  í‘œì‹ ë¦¬ìŠ¤íŠ¸ (ì˜µì…˜, Noneì´ë©´ ëª¨ë“  KABOAT í´ë˜ìŠ¤ ì‚¬ìš©)

        Returns:
            detections: íƒì§€ ê²°ê³¼
            image: PIL Image
            inference_time: ì¶”ë¡  ì‹œê°„
        """
        start_time = time.time()

        # ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(image_path).convert("RGB")

        # 1ï¸âƒ£ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë°©ì‹
        if self.use_text_prompt and hasattr(self.model, 'set_classes'):
            if text_queries is not None:
                # íŠ¹ì • ì¿¼ë¦¬ë§Œ íƒì§€
                self.model.set_classes(text_queries)
            else:
                # ëª¨ë“  KABOAT í´ë˜ìŠ¤ íƒì§€
                self.model.set_classes(self.kaboat_classes)

        # YOLOEë¡œ íƒì§€ ìˆ˜í–‰
        results = self.model.predict(
            source=image_path,
            conf=self.threshold,
            device=self.device,
            verbose=False,
            imgsz=640  # ì´ë¯¸ì§€ í¬ê¸° (ë¹ ë¥¸ ì¶”ë¡ : 640, ì •í™•ë„: 1280)
        )

        inference_time = time.time() - start_time

        # ê²°ê³¼ íŒŒì‹±
        detections = []

        if len(results) > 0 and results[0].boxes is not None:
            boxes = results[0].boxes

            for i in range(len(boxes)):
                # ë°”ìš´ë”© ë°•ìŠ¤ (xyxy í˜•ì‹)
                bbox = boxes.xyxy[i].cpu().numpy()
                confidence = float(boxes.conf[i].cpu().numpy())

                # í´ë˜ìŠ¤ ID ë° ì´ë¦„
                class_id = int(boxes.cls[i].cpu().numpy())

                # í´ë˜ìŠ¤ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
                if hasattr(self.model, 'names') and class_id < len(self.model.names):
                    label = self.model.names[class_id]
                else:
                    # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ëª¨ë“œì¸ ê²½ìš°
                    query_list = text_queries if text_queries else self.kaboat_classes
                    label = query_list[class_id] if class_id < len(query_list) else f"object_{class_id}"

                if confidence >= self.threshold:
                    x1, y1, x2, y2 = bbox

                    center_x = (x1 + x2) / 2
                    center_y = (y1 + y2) / 2
                    width = x2 - x1
                    height = y2 - y1

                    detections.append({
                        "label": label,
                        "confidence": float(confidence),
                        "bbox": [float(x1), float(y1), float(x2), float(y2)],
                        "center": [float(center_x), float(center_y)],
                        "width": float(width),
                        "height": float(height),
                        "class_id": class_id
                    })

        return detections, image, inference_time

    def calculate_navigation_command(self, detection, image_size, tolerance=50):
        """í•­ë²• ëª…ë ¹ ìƒì„±"""
        if not detection:
            return "NO TARGET FOUND - SEARCH"

        img_width, img_height = image_size
        img_center_x = img_width / 2
        img_center_y = img_height / 2

        marker_x, marker_y = detection["center"]
        marker_width = detection["width"]

        distance_ratio = marker_width / img_width
        horizontal_error = marker_x - img_center_x
        vertical_error = marker_y - img_center_y

        if abs(horizontal_error) > tolerance:
            if horizontal_error > 0:
                return f"MOVE LEFT ({int(horizontal_error)}px)"
            else:
                return f"MOVE RIGHT ({int(abs(horizontal_error))}px)"

        if distance_ratio < 0.15:
            return f"MOVE FORWARD (ê±°ë¦¬: {distance_ratio*100:.1f}%)"
        elif distance_ratio > 0.35:
            return f"MOVE BACK (ê±°ë¦¬: {distance_ratio*100:.1f}%)"
        else:
            if abs(vertical_error) > tolerance:
                if vertical_error > 0:
                    return f"ADJUST UP ({int(vertical_error)}px)"
                else:
                    return f"ADJUST DOWN ({int(abs(vertical_error))}px)"

            return f"âœ“ READY TO DOCK (ê±°ë¦¬: {distance_ratio*100:.1f}%)"

    def visualize_detection(self, image, detections, command, inference_time, save_path=None):
        """íƒì§€ ê²°ê³¼ ì‹œê°í™”"""
        draw = ImageDraw.Draw(image)

        img_width, img_height = image.size
        center_x, center_y = img_width / 2, img_height / 2

        # ì¤‘ì‹¬ ì‹­ìì„ 
        cross_size = 30
        draw.line([(center_x - cross_size, center_y), (center_x + cross_size, center_y)],
                  fill="yellow", width=3)
        draw.line([(center_x, center_y - cross_size), (center_x, center_y + cross_size)],
                  fill="yellow", width=3)

        # ìƒ‰ìƒ ë§¤í•‘
        color_map = {
            "red": "red",
            "green": "lime",
            "blue": "cyan",
            "orange": "orange",
            "yellow": "yellow",
            "buoy": "magenta",
            "cross": "white",
        }

        # íƒì§€ëœ ê°ì²´ í‘œì‹œ (ìµœëŒ€ 10ê°œ)
        for idx, det in enumerate(detections[:10]):
            bbox = det["bbox"]
            label = det["label"]
            conf = det["confidence"]

            box_color = "red"
            for color_name, color_value in color_map.items():
                if color_name in label.lower():
                    box_color = color_value
                    break

            width = 4 if idx == 0 else 2
            draw.rectangle(bbox, outline=box_color, width=width)

            if idx == 0:
                cx, cy = det["center"]
                draw.ellipse([cx-6, cy-6, cx+6, cy+6], fill=box_color, outline="white")

            text = f"{label}: {conf:.2f}"
            text_bbox = draw.textbbox((bbox[0], bbox[1]-22), text)
            draw.rectangle(text_bbox, fill="black")
            draw.text((bbox[0], bbox[1]-20), text, fill=box_color)

        # ëª…ë ¹ ë° ì„±ëŠ¥ ì •ë³´
        info_text = f"{command} | {inference_time*1000:.0f}ms | YOLOE"
        info_bbox = draw.textbbox((8, 8), info_text)
        draw.rectangle(info_bbox, fill="black")
        draw.text((10, 10), info_text, fill="lime")

        if save_path:
            image.save(save_path)

        return image

    def batch_test(self, input_dir, output_dir, target_queries=None):
        """ë°°ì¹˜ í…ŒìŠ¤íŠ¸"""
        os.makedirs(output_dir, exist_ok=True)
        vis_dir = os.path.join(output_dir, "visualizations_yoloe")
        os.makedirs(vis_dir, exist_ok=True)

        valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp'}
        image_files = [
            f for f in os.listdir(input_dir)
            if os.path.splitext(f.lower())[1] in valid_extensions
        ]

        if not image_files:
            print(f"ê²½ê³ : {input_dir}ì— ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # KABOAT 2025 ì‹¤ì œ í‘œì‹ ì •ì˜
        if target_queries is None:
            target_queries = []

            # 1. ë„í‚¹ ì„ë¬´: í˜•ìƒ(ì‚¼ê°í˜•, ì›í˜•, ë„¤ëª¨) Ã— ìƒ‰ìƒ(ë¹¨ê°•, ì´ˆë¡, íŒŒë‘, ì£¼í™©, ë…¸ë‘)
            docking_shapes = ["triangle", "circle", "square"]
            docking_colors = ["red", "green", "blue", "orange", "yellow"]
            target_queries.extend([f"{color} {shape}" for color in docking_colors for shape in docking_shapes])

            # 2. ì¥ì• ë¬¼ íšŒí”¼: ì£¼í™©ìƒ‰ ë¶€í‘œ/ì›ë¿”
            target_queries.extend([
                "orange buoy", "orange cone", "orange marker"
            ])

            # 3. ìœ„ì¹˜ìœ ì§€: ë…¸ë€ìƒ‰ ë¶€í‘œ
            target_queries.extend([
                "yellow buoy", "yellow marker", "yellow waypoint"
            ])

            # 4. íƒìƒ‰: ë¹¨ê°•/ì´ˆë¡/íŒŒë‘ ë¶€í‘œ
            target_queries.extend([
                "red buoy", "green buoy", "blue buoy"
            ])

            # 5. í•­ë¡œì¶”ì¢…: ë¹¨ê°•/ì´ˆë¡ ê²Œì´íŠ¸ ë¶€í‘œ
            target_queries.extend([
                "red navigation buoy", "green navigation buoy",
                "red gate marker", "green gate marker"
            ])

        print(f"\n{'='*80}")
        print(f"ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print(f"{'='*80}")
        print(f"ì´ë¯¸ì§€ ìˆ˜: {len(image_files)}")
        print(f"í‘œì‹ ìˆ˜: {len(target_queries)}")
        print(f"{'='*80}\n")

        results = []
        total_time = 0

        for idx, image_file in enumerate(image_files, 1):
            image_path = os.path.join(input_dir, image_file)
            print(f"[{idx}/{len(image_files)}] {image_file}")

            try:
                detections, image, inference_time = self.detect_markers(
                    image_path, target_queries
                )
                total_time += inference_time

                detections_sorted = sorted(detections, key=lambda x: x["confidence"], reverse=True)
                best_detection = detections_sorted[0] if detections_sorted else None

                command = self.calculate_navigation_command(best_detection, image.size)

                result = {
                    "image_file": image_file,
                    "image_path": image_path,
                    "detections": detections_sorted,
                    "best_detection": best_detection,
                    "navigation_command": command,
                    "inference_time_ms": inference_time * 1000,
                    "timestamp": datetime.now().isoformat()
                }

                results.append(result)

                print(f"  íƒì§€: {len(detections_sorted)}ê°œ")
                print(f"  ì‹œê°„: {inference_time*1000:.0f}ms")
                if detections_sorted:
                    print(f"  TOP3:")
                    for i, det in enumerate(detections_sorted[:3], 1):
                        print(f"    {i}. {det['label']}: {det['confidence']:.3f}")
                    print(f"  â†’ {command}")
                else:
                    print(f"  â†’ {command}")

                vis_path = os.path.join(vis_dir, f"yoloe_{image_file}")
                self.visualize_detection(image, detections_sorted, command, inference_time, vis_path)
                print(f"  ì €ì¥: {vis_path}")
                print("-" * 80)

            except Exception as e:
                print(f"ì˜¤ë¥˜: {str(e)}")
                import traceback
                traceback.print_exc()
                results.append({
                    "image_file": image_file,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                })

        # ì„±ëŠ¥ í†µê³„
        avg_time = total_time / len(image_files) if image_files else 0
        fps = 1 / avg_time if avg_time > 0 else 0

        print(f"\n{'='*80}")
        print(f"YOLOE ì„±ëŠ¥ í†µê³„")
        print(f"{'='*80}")
        print(f"í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time*1000:.0f}ms")
        print(f"ì˜ˆìƒ FPS: {fps:.1f}")
        print(f"{'='*80}\n")

        # ê²°ê³¼ ì €ì¥
        output_file = os.path.join(
            output_dir,
            f"yoloe_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "results": results,
                "performance": {
                    "avg_inference_time_ms": avg_time * 1000,
                    "fps": fps,
                    "total_images": len(image_files),
                    "model": "YOLOE"
                }
            }, f, ensure_ascii=False, indent=2)

        print(f"ê²°ê³¼ ì €ì¥: {output_file}")
        print(f"ì‹œê°í™”: {vis_dir}\n")

        return results

    def process_video(self, video_path, output_path, target_queries=None, display=False):
        """ë¹„ë””ì˜¤ ì‹¤ì‹œê°„ íƒì§€

        Args:
            video_path: ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
            output_path: ì¶œë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
            target_queries: íƒì§€í•  í‘œì‹ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ KABOAT í´ë˜ìŠ¤ ì‚¬ìš©)
            display: ì‹¤ì‹œê°„ í™”ë©´ í‘œì‹œ ì—¬ë¶€

        Returns:
            dict: ì²˜ë¦¬ ê²°ê³¼ í†µê³„
        """
        print(f"\n{'='*80}")
        print(f"ë¹„ë””ì˜¤ ì‹¤ì‹œê°„ íƒì§€ ì‹œì‘")
        print(f"{'='*80}")
        print(f"ì…ë ¥: {video_path}")
        print(f"ì¶œë ¥: {output_path}")

        # ë¹„ë””ì˜¤ ì—´ê¸°
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"ì˜¤ë¥˜: ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            return None

        # ë¹„ë””ì˜¤ ì†ì„±
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        print(f"í•´ìƒë„: {width}x{height}")
        print(f"FPS: {fps}")
        print(f"ì´ í”„ë ˆì„: {total_frames}")
        print(f"{'='*80}\n")

        # ì¶œë ¥ ë¹„ë””ì˜¤ ì„¤ì •
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        # íƒì§€ ì¿¼ë¦¬ ì„¤ì •
        if target_queries is None:
            target_queries = []
            docking_shapes = ["triangle", "circle", "square"]
            docking_colors = ["red", "green", "blue", "orange", "yellow"]
            target_queries.extend([f"{color} {shape}" for color in docking_colors for shape in docking_shapes])
            target_queries.extend([
                "orange buoy", "orange cone", "orange marker",
                "yellow buoy", "yellow marker",
                "red buoy", "green buoy", "blue buoy"
            ])

        # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        if self.use_text_prompt and hasattr(self.model, 'set_classes'):
            self.model.set_classes(target_queries)

        # í”„ë ˆì„ ì²˜ë¦¬
        frame_count = 0
        total_inference_time = 0
        detection_stats = []

        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                frame_count += 1
                start_time = time.time()

                # BGR -> RGB ë³€í™˜ (YOLOëŠ” RGB ì‚¬ìš©)
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # YOLO íƒì§€ (NumPy ë°°ì—´ ì§ì ‘ ì…ë ¥)
                results = self.model.predict(
                    source=frame_rgb,
                    conf=self.threshold,
                    device=self.device,
                    verbose=False,
                    imgsz=640
                )

                inference_time = time.time() - start_time
                total_inference_time += inference_time

                # íƒì§€ ê²°ê³¼ íŒŒì‹±
                detections = []
                if len(results) > 0 and results[0].boxes is not None:
                    boxes = results[0].boxes
                    for i in range(len(boxes)):
                        bbox = boxes.xyxy[i].cpu().numpy()
                        confidence = float(boxes.conf[i].cpu().numpy())
                        class_id = int(boxes.cls[i].cpu().numpy())

                        query_list = target_queries if target_queries else self.kaboat_classes
                        label = query_list[class_id] if class_id < len(query_list) else f"object_{class_id}"

                        if confidence >= self.threshold:
                            x1, y1, x2, y2 = bbox
                            center_x = (x1 + x2) / 2
                            center_y = (y1 + y2) / 2
                            width = x2 - x1
                            height = y2 - y1

                            detections.append({
                                "label": label,
                                "confidence": float(confidence),
                                "bbox": [float(x1), float(y1), float(x2), float(y2)],
                                "center": [float(center_x), float(center_y)],
                                "width": float(width),
                                "height": float(height),
                                "class_id": class_id
                            })

                # íƒì§€ ê²°ê³¼ ì •ë ¬
                detections_sorted = sorted(detections, key=lambda x: x["confidence"], reverse=True)
                best_detection = detections_sorted[0] if detections_sorted else None

                # í•­ë²• ëª…ë ¹
                command = self.calculate_navigation_command(best_detection, (width, height))

                # í”„ë ˆì„ì— ì‹œê°í™”
                frame = self._draw_detections_on_frame(
                    frame, detections_sorted, command, inference_time, frame_count, total_frames
                )

                # ë¹„ë””ì˜¤ ì €ì¥
                out.write(frame)

                # í†µê³„ ì €ì¥
                detection_stats.append({
                    "frame": frame_count,
                    "detections": len(detections_sorted),
                    "inference_time_ms": inference_time * 1000,
                    "command": command
                })

                # ì‹¤ì‹œê°„ í‘œì‹œ
                if display:
                    cv2.imshow('YOLOE Real-time Detection', frame)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break

                # ì§„í–‰ ìƒí™© ì¶œë ¥ (10% ë‹¨ìœ„)
                if frame_count % max(1, total_frames // 10) == 0:
                    progress = (frame_count / total_frames) * 100
                    avg_fps = frame_count / total_inference_time if total_inference_time > 0 else 0
                    print(f"ì§„í–‰: {progress:.0f}% ({frame_count}/{total_frames}) | "
                          f"í‰ê·  FPS: {avg_fps:.1f} | íƒì§€: {len(detections_sorted)}ê°œ")

        finally:
            cap.release()
            out.release()
            if display:
                cv2.destroyAllWindows()

        # ìµœì¢… í†µê³„
        avg_inference_time = total_inference_time / frame_count if frame_count > 0 else 0
        avg_fps = frame_count / total_inference_time if total_inference_time > 0 else 0

        print(f"\n{'='*80}")
        print(f"ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ")
        print(f"{'='*80}")
        print(f"ì²˜ë¦¬ í”„ë ˆì„: {frame_count}/{total_frames}")
        print(f"í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_inference_time*1000:.0f}ms")
        print(f"í‰ê·  FPS: {avg_fps:.1f}")
        print(f"ì¶œë ¥ ì €ì¥: {output_path}")
        print(f"{'='*80}\n")

        return {
            "total_frames": frame_count,
            "avg_inference_time_ms": avg_inference_time * 1000,
            "avg_fps": avg_fps,
            "detection_stats": detection_stats
        }

    def _draw_detections_on_frame(self, frame, detections, command, inference_time, frame_num, total_frames):
        """í”„ë ˆì„ì— íƒì§€ ê²°ê³¼ ê·¸ë¦¬ê¸°"""
        height, width = frame.shape[:2]
        center_x, center_y = width // 2, height // 2

        # ì¤‘ì‹¬ ì‹­ìì„ 
        cv2.line(frame, (center_x - 30, center_y), (center_x + 30, center_y), (0, 255, 255), 2)
        cv2.line(frame, (center_x, center_y - 30), (center_x, center_y + 30), (0, 255, 255), 2)

        # ìƒ‰ìƒ ë§¤í•‘
        color_map = {
            "red": (0, 0, 255),
            "green": (0, 255, 0),
            "blue": (255, 0, 0),
            "orange": (0, 165, 255),
            "yellow": (0, 255, 255),
            "buoy": (255, 0, 255),
        }

        # íƒì§€ëœ ê°ì²´ í‘œì‹œ
        for idx, det in enumerate(detections[:10]):
            x1, y1, x2, y2 = [int(v) for v in det["bbox"]]
            label = det["label"]
            conf = det["confidence"]

            # ìƒ‰ìƒ ì„ íƒ
            box_color = (0, 0, 255)  # ê¸°ë³¸: ë¹¨ê°•
            for color_name, color_value in color_map.items():
                if color_name in label.lower():
                    box_color = color_value
                    break

            # ë°”ìš´ë”© ë°•ìŠ¤
            thickness = 3 if idx == 0 else 2
            cv2.rectangle(frame, (x1, y1), (x2, y2), box_color, thickness)

            # ì¤‘ì‹¬ì  (ìµœìš°ì„  íƒ€ê²Ÿë§Œ)
            if idx == 0:
                cx, cy = [int(v) for v in det["center"]]
                cv2.circle(frame, (cx, cy), 6, box_color, -1)
                cv2.circle(frame, (cx, cy), 8, (255, 255, 255), 2)

            # ë¼ë²¨
            text = f"{label}: {conf:.2f}"
            (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
            cv2.rectangle(frame, (x1, y1 - text_height - 4), (x1 + text_width, y1), (0, 0, 0), -1)
            cv2.putText(frame, text, (x1, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 2)

        # ìƒë‹¨ ì •ë³´: ëª…ë ¹ + ì„±ëŠ¥
        info_text = f"{command} | {inference_time*1000:.0f}ms | FPS: {1/inference_time:.1f}"
        cv2.rectangle(frame, (5, 5), (width - 5, 35), (0, 0, 0), -1)
        cv2.putText(frame, info_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

        # í•˜ë‹¨ ì •ë³´: í”„ë ˆì„ ë²ˆí˜¸
        frame_text = f"Frame: {frame_num}/{total_frames}"
        cv2.putText(frame, frame_text, (10, height - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

        return frame


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys

    test_img_dir = "/home/yune/Kaboat2025_sim/test_img"
    test_vla_dir = "/home/yune/Kaboat2025_sim/test_vla"
    test_video_dir = "/home/yune/Kaboat2025_sim/test_video"

    # ëª¨ë“œ ì„ íƒ: ì´ë¯¸ì§€ ë˜ëŠ” ë¹„ë””ì˜¤
    mode = "video" if len(sys.argv) > 1 and sys.argv[1] == "video" else "image"

    print("\n" + "=" * 80)
    print("YOLOE KABOAT ë„í‚¹ ì‹œìŠ¤í…œ [ICCV 2025]")
    print("=" * 80)
    print("\n[KABOAT 2025 ì„ë¬´]")
    print("ğŸ¯ ë„í‚¹: ì‚¼ê°í˜•/ì›í˜•/ë„¤ëª¨ Ã— 5ê°€ì§€ ìƒ‰ìƒ")
    print("ğŸŸ  ì¥ì• ë¬¼ íšŒí”¼: ì£¼í™©ìƒ‰ ë¶€í‘œ/ì›ë¿”")
    print("ğŸŸ¡ ìœ„ì¹˜ìœ ì§€: ë…¸ë€ìƒ‰ ë¶€í‘œ")
    print("ğŸ”´ğŸŸ¢ğŸ”µ íƒìƒ‰: ë¹¨ê°•/ì´ˆë¡/íŒŒë‘ ë¶€í‘œ")
    print("ğŸš¢ í•­ë¡œì¶”ì¢…: ë¹¨ê°•/ì´ˆë¡ ê²Œì´íŠ¸ ë¶€í‘œ\n")

    print("[YOLOE íŠ¹ì§•]")
    print("âœ“ Real-Time Seeing Anything")
    print("âœ“ 3ê°€ì§€ í”„ë¡¬í”„íŠ¸ ë°©ì‹ (í…ìŠ¤íŠ¸/ë¹„ì£¼ì–¼/í”„ë¡¬í”„íŠ¸ ì—†ìŒ)")
    print("âœ“ YOLO-Worldv2 ëŒ€ë¹„ 1.4ë°° ë¹ ë¥¸ ì¶”ë¡ ")
    print("âœ“ Zero-shot ì„±ëŠ¥ SOTA\n")

    # YOLOE íƒì§€ê¸° ì´ˆê¸°í™”
    # ëª¨ë¸ ì„ íƒ:
    # - yolov8l.pt: ê¸°ë³¸ YOLO (COCO í´ë˜ìŠ¤ë§Œ)
    # - yolov8l-world.pt: YOLO-World (í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì§€ì›)
    # - yoloe-*.pt: YOLOE ê³µì‹ ëª¨ë¸ (ê¶Œì¥)
    detector = YOLOEDockingDetector(
        model_name="yoloe-v8l-seg.pt",  # TODO: yoloe-v8l.ptë¡œ êµì²´
        device="cuda",
        confidence_threshold=0.25,
        use_text_prompt=True  # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    )

    if mode == "video":
        # ë¹„ë””ì˜¤ ì²˜ë¦¬ ëª¨ë“œ
        print("\n[ëª¨ë“œ: ë¹„ë””ì˜¤ ì‹¤ì‹œê°„ íƒì§€]\n")

        # test_video ë””ë ‰í† ë¦¬ ìƒì„± (ì—†ìœ¼ë©´)
        os.makedirs(test_video_dir, exist_ok=True)

        # ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
        video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv'}
        video_files = [
            f for f in os.listdir(test_video_dir)
            if os.path.splitext(f.lower())[1] in video_extensions
        ] if os.path.exists(test_video_dir) else []

        if not video_files:
            print(f"ê²½ê³ : {test_video_dir}ì— ë¹„ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            print(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ {test_video_dir}ì— ì¶”ê°€í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
            print(f"ì§€ì› í˜•ì‹: {', '.join(video_extensions)}")
            return

        # ê° ë¹„ë””ì˜¤ ì²˜ë¦¬
        for video_file in video_files:
            video_path = os.path.join(test_video_dir, video_file)
            output_name = f"yoloe_detected_{os.path.splitext(video_file)[0]}.mp4"
            output_path = os.path.join(test_vla_dir, output_name)

            print(f"\nì²˜ë¦¬ ì¤‘: {video_file}")
            stats = detector.process_video(
                video_path=video_path,
                output_path=output_path,
                target_queries=None,
                display=True  # Trueë¡œ ë³€ê²½í•˜ë©´ ì‹¤ì‹œê°„ í™”ë©´ í‘œì‹œ
            )

            if stats:
                print(f"âœ“ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ: {output_path}")

        print("\n" + "=" * 80)
        print(f"âœ“ ì „ì²´ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ: {len(video_files)}ê°œ")
        print("=" * 80)

    else:
        # ì´ë¯¸ì§€ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ
        print("\n[ëª¨ë“œ: ì´ë¯¸ì§€ ë°°ì¹˜ ì²˜ë¦¬]\n")
        print("ğŸ’¡ ë¹„ë””ì˜¤ ì²˜ë¦¬ ëª¨ë“œ: python test_yoloe_docking.py video\n")

        if not os.path.exists(test_img_dir):
            print(f"ì˜¤ë¥˜: {test_img_dir} ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        # ë°°ì¹˜ í…ŒìŠ¤íŠ¸
        results = detector.batch_test(
            test_img_dir,
            test_vla_dir,
            target_queries=None
        )

        print("=" * 80)
        print(f"âœ“ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ì´ë¯¸ì§€")
        print("=" * 80)


if __name__ == "__main__":
    main()
