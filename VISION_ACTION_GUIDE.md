# Vision-to-Action ì§ì ‘ í•™ìŠµ ê°€ì´ë“œ

## ê°œìš”

**ì´ë¯¸ì§€ ì…ë ¥ â†’ ì§ì ‘ ëª¨í„° ê°’ ì¶œë ¥**

VLMì˜ í…ìŠ¤íŠ¸ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³  End-to-Endë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

```
ì¹´ë©”ë¼ ì´ë¯¸ì§€ â†’ [ì „ì§„ê°’, ì¢Œìš°ê°’, ì„ íšŒê°’]
               â†‘
             ë²”ìœ„: -1 ~ 1
```

## ì¥ì  vs VLM ë°©ì‹

| í•­ëª© | Vision-to-Action | VLM (Qwen/Phi-3) |
|------|-----------------|------------------|
| **ì²˜ë¦¬ ë‹¨ê³„** | 1ë‹¨ê³„ (ì§ì ‘) | 2ë‹¨ê³„ (í…ìŠ¤íŠ¸â†’ë³€í™˜) |
| **ëª¨ë¸ í¬ê¸°** | ~100MB | 4-7GB |
| **GPU ë©”ëª¨ë¦¬** | 1-2GB | 4-8GB |
| **ì¶”ë¡  ì†ë„** | **ë§¤ìš° ë¹ ë¦„** (10-50ms) | ëŠë¦¼ (500ms-2s) |
| **ì •í™•ë„** | ë°ì´í„° í’ˆì§ˆì— ì˜ì¡´ | ë†’ìŒ (ì‚¬ì „í•™ìŠµ) |
| **í•™ìŠµ ë‚œì´ë„** | **ì‰¬ì›€** | ì–´ë ¤ì›€ |

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1ë‹¨ê³„: ì‹¤í–‰

```bash
python train_vision_action.py
```

**ì¶œë ¥**:
```
Vision-to-Action ì§ì ‘ í•™ìŠµ
ìƒ˜í”Œ ë°ì´í„° ìƒì„±: vision_action_data.json
í•™ìŠµ ì‹œì‘ (100 epochs)...
Epoch 10/100, Loss: 0.023456
...
ëª¨ë¸ ì €ì¥ ì™„ë£Œ: vision_action_model.pt

=== ì¶”ë¡  í…ŒìŠ¤íŠ¸ ===
Screenshot from 2025-10-03 10-15-26.png
  â†’ ì•¡ì…˜: [+0.300, +0.000, +0.500]
     ì „ì§„: +0.300 | ì¢Œìš°: +0.000 | ì„ íšŒ: +0.500
```

### 2ë‹¨ê³„: ROS ë…¸ë“œ ì‹¤í–‰

```bash
python3 vision_action_ros_node.py
```

## ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ë°©ë²•

### ë°ì´í„°ì…‹ í˜•ì‹ (vision_action_data.json):

```json
[
  {
    "image": "data/img_001.jpg",
    "action": [0.5, 0.0, 0.3]
  },
  {
    "image": "data/img_002.jpg",
    "action": [0.0, 0.0, 0.0]
  }
]
```

**action ê°’**:
- `[0]`: ì „ì§„ (-1=í›„ì§„, 0=ì •ì§€, 1=ì „ì§„)
- `[1]`: ì¢Œìš° (-1=ì™¼ìª½, 0=ì¤‘ë¦½, 1=ì˜¤ë¥¸ìª½)
- `[2]`: ì„ íšŒ (-1=ì¢ŒíšŒì „, 0=ì§ì§„, 1=ìš°íšŒì „)

### ë°ì´í„° ìˆ˜ì§‘ ì „ëµ

#### ë°©ë²• 1: ìˆ˜ë™ ë¼ë²¨ë§ (ì´ˆê¸°)

```python
# ì‹œë®¬ë ˆì´í„°ì—ì„œ ì´ë¯¸ì§€ ìº¡ì²˜ í›„ ìˆ˜ë™ìœ¼ë¡œ ë¼ë²¨ë§
data = []

# ì˜ˆì‹œ
data.append({
    "image": "captures/dock_left.jpg",
    "action": [0.3, 0.0, 0.5]  # ì „ì§„í•˜ë©° ìš°íšŒì „
})

data.append({
    "image": "captures/dock_right.jpg",
    "action": [0.3, 0.0, -0.5]  # ì „ì§„í•˜ë©° ì¢ŒíšŒì „
})

data.append({
    "image": "captures/dock_center_far.jpg",
    "action": [0.7, 0.0, 0.0]  # ì „ì§„
})

data.append({
    "image": "captures/dock_center_near.jpg",
    "action": [0.0, 0.0, 0.0]  # ì •ì§€
})
```

#### ë°©ë²• 2: ì‹œë®¬ë ˆì´í„° ìë™ ìˆ˜ì§‘

```python
# ROS2ì—ì„œ ìë™ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘
class DataCollector(Node):
    def __init__(self):
        self.image_sub = self.create_subscription(
            Image, '/camera/image_raw', self.save_image, 10)
        self.cmd_sub = self.create_subscription(
            Twist, '/cmd_vel', self.save_action, 10)

        self.data = []

    def save_image(self, msg):
        # ì´ë¯¸ì§€ ì €ì¥
        cv_image = self.bridge.imgmsg_to_cv2(msg)
        cv2.imwrite(f'data/img_{self.count}.jpg', cv_image)

    def save_action(self, msg):
        # ì•¡ì…˜ ì €ì¥
        self.data.append({
            "image": f'data/img_{self.count}.jpg',
            "action": [
                msg.linear.x,
                msg.linear.y,
                msg.angular.z
            ]
        })
```

#### ë°©ë²• 3: ì „ë¬¸ê°€ ì‹œì—° (Imitation Learning)

1. ì‚¬ëŒì´ ì¡°ì¢…í•˜ë©° ê¸°ë¡
2. ì´ë¯¸ì§€ + ì¡°ì´ìŠ¤í‹± ì…ë ¥ ì €ì¥
3. ìë™ìœ¼ë¡œ ë°ì´í„°ì…‹ ìƒì„±

### í•„ìš” ë°ì´í„° ì–‘

- **ìµœì†Œ**: 50-100ê°œ (í…ŒìŠ¤íŠ¸ìš©)
- **ê¶Œì¥**: 500-1000ê°œ (ì‹¤ì „ìš©)
- **ì´ìƒì **: 5000ê°œ ì´ìƒ (í”„ë¡œë•ì…˜)

**ë¶„í¬**:
- ë‹¤ì–‘í•œ ê±°ë¦¬ (ê°€ê¹Œì›€/ì¤‘ê°„/ë©€ë¦¬)
- ë‹¤ì–‘í•œ ê°ë„ (ì™¼ìª½/ì¤‘ì•™/ì˜¤ë¥¸ìª½)
- ë‹¤ì–‘í•œ ì¡°ëª… (ë‚®/ë°¤/íë¦¼)

## ğŸ”§ í•™ìŠµ íŒŒë¼ë¯¸í„° ì¡°ì •

### ê¸°ë³¸ ì„¤ì •

```python
train(
    data_path="vision_action_data.json",
    epochs=100,        # í•™ìŠµ ë°˜ë³µ íšŸìˆ˜
    batch_size=4,      # ë°°ì¹˜ í¬ê¸°
)
```

### ê³ ê¸‰ ì„¤ì •

```python
# ëª¨ë¸ í¬ê¸° ì¡°ì •
class VisionActionNetwork(nn.Module):
    def __init__(self, backbone='resnet50'):
        # resnet18: ê°€ë²¼ì›€, ë¹ ë¦„
        # resnet50: ê· í˜•
        # resnet101: ì •í™•, ëŠë¦¼

        if backbone == 'resnet18':
            resnet = models.resnet18(pretrained=True)
            feature_dim = 512
        elif backbone == 'resnet50':
            resnet = models.resnet50(pretrained=True)
            feature_dim = 2048
```

### í•™ìŠµë¥  ì¡°ì •

```python
# ë¹ ë¥¸ í•™ìŠµ (ë¶ˆì•ˆì • ê°€ëŠ¥)
optimizer = optim.Adam(model.parameters(), lr=1e-3)

# ì•ˆì •ì  í•™ìŠµ (ì¶”ì²œ)
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# ëŠë¦¬ì§€ë§Œ ì •í™•
optimizer = optim.Adam(model.parameters(), lr=1e-5)
```

## ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ íŒ

### 1. Data Augmentation

```python
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=0.5),  # ì¢Œìš° ë°˜ì „
    transforms.ColorJitter(brightness=0.2),  # ë°ê¸° ë³€í™”
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225])
])
```

### 2. ê²€ì¦ ë°ì´í„° ë¶„ë¦¬

```python
# 80% í•™ìŠµ, 20% ê²€ì¦
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = torch.utils.data.random_split(
    dataset, [train_size, val_size]
)
```

### 3. Early Stopping

```python
best_loss = float('inf')
patience = 10
patience_counter = 0

for epoch in range(epochs):
    train_loss = train_epoch()
    val_loss = validate()

    if val_loss < best_loss:
        best_loss = val_loss
        torch.save(model.state_dict(), 'best_model.pt')
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print("Early stopping!")
            break
```

## ğŸ¯ ROS2 í†µí•©

### ìƒì„±ëœ ROS ë…¸ë“œ ì‚¬ìš©

```bash
# 1. ë…¸ë“œ ì‹¤í–‰
python3 vision_action_ros_node.py

# 2. ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ ì¹´ë©”ë¼ í† í”½ í™•ì¸
ros2 topic echo /cmd_vel
```

### í† í”½ ì •ë³´

**êµ¬ë…**:
- `/camera/image_raw` (sensor_msgs/Image)

**ë°œí–‰**:
- `/cmd_vel` (geometry_msgs/Twist)
  - `linear.x`: ì „ì§„ (-1~1)
  - `linear.y`: ì¢Œìš° (-1~1)
  - `angular.z`: ì„ íšŒ (-1~1)

### ì•ˆì „ ì œí•œ ì¶”ê°€

```python
def limit_action(action, max_speed=0.5):
    """ì•¡ì…˜ ê°’ ì œí•œ"""
    action = np.clip(action, -1, 1)  # -1~1 ë²”ìœ„
    action = action * max_speed       # ìµœëŒ€ ì†ë„ ì œí•œ
    return action

# ì‚¬ìš©
action = model(image)
action = limit_action(action, max_speed=0.5)
```

## ğŸ” ë””ë²„ê¹…

### ê³¼ì í•© í™•ì¸

```python
# í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° Loss ë¹„êµ
if train_loss < 0.01 and val_loss > 0.1:
    print("ê³¼ì í•© ë°œìƒ! ë°ì´í„° ì¦ê°• ë˜ëŠ” Dropout ì¦ê°€")
```

### í•™ìŠµ ì•ˆ ë¨

```python
# í•™ìŠµë¥ ì´ ë„ˆë¬´ ë‚®ê±°ë‚˜ ë†’ìŒ
# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì‚¬ìš©
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5
)
```

### ì˜ˆì¸¡ ê°’ì´ ê·¹ë‹¨ì 

```python
# Tanh ëŒ€ì‹  ë‹¤ë¥¸ í™œì„±í™” í•¨ìˆ˜
nn.Sigmoid()  # 0~1 ì¶œë ¥ í›„ ìŠ¤ì¼€ì¼ ì¡°ì •
# ë˜ëŠ”
# ì¶œë ¥ ë ˆì´ì–´ì— í´ë¦¬í•‘ ì¶”ê°€
```

## ğŸ“Š ì„±ëŠ¥ í‰ê°€

### í‰ê°€ ë©”íŠ¸ë¦­

```python
def evaluate(model, dataloader):
    model.eval()
    total_mae = 0  # Mean Absolute Error

    with torch.no_grad():
        for images, actions in dataloader:
            pred = model(images)
            mae = torch.abs(pred - actions).mean()
            total_mae += mae.item()

    return total_mae / len(dataloader)
```

### ì‹¤ì œ í™˜ê²½ í…ŒìŠ¤íŠ¸

1. **ì‹œë®¬ë ˆì´í„° í…ŒìŠ¤íŠ¸** (ë¨¼ì €)
2. **ì‹¤ì œ í™˜ê²½ í…ŒìŠ¤íŠ¸** (ë‚˜ì¤‘)
3. **ì•ˆì „ ê±°ë¦¬ ìœ ì§€** (í•­ìƒ)

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

### ë‹¨ê³„ë³„ ì ìš©

**1ì£¼ì°¨**: ê¸°ë³¸ í•™ìŠµ
```bash
# 50-100ê°œ ìƒ˜í”Œ ë°ì´í„°
python train_vision_action.py
```

**2ì£¼ì°¨**: ë°ì´í„° í™•ì¥
```bash
# 500-1000ê°œ ë°ì´í„° ìˆ˜ì§‘
# Fine-tuning
```

**3ì£¼ì°¨**: ì‹¤ì „ ë°°í¬
```bash
# ROS ë…¸ë“œ í†µí•©
python3 vision_action_ros_node.py
```

## ğŸ“š ì°¸ê³ 

**ìƒì„±ëœ íŒŒì¼**:
- `train_vision_action.py` - í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- `vision_action_model.pt` - í•™ìŠµëœ ëª¨ë¸
- `vision_action_ros_node.py` - ROS2 ë…¸ë“œ
- `vision_action_data.json` - í•™ìŠµ ë°ì´í„°

**ì˜ì¡´ì„±**:
```bash
pip install torch torchvision pillow
```

**ëª¨ë¸ ë¹„êµ**:
- Vision-to-Action: ë¹ ë¦„, ê°€ë²¼ì›€, ë°ì´í„° ì˜ì¡´ì 
- VLM (Qwen/Phi-3): ëŠë¦¼, ë¬´ê±°ì›€, ë²”ìš©ì 
- OpenVLA: ë§¤ìš° ë¬´ê±°ì›€, ë¡œë´‡ ì¡°ì‘ íŠ¹í™”

**ì¶”ì²œ**: ì†Œê·œëª¨ í”„ë¡œì íŠ¸ì—ëŠ” **Vision-to-Action**ì´ ìµœì !
