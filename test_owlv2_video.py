"""
OWLv2 ë¹„ë””ì˜¤ ì‹¤ì‹œê°„ íƒì§€
íŒŒë€ +ëª¨ì–‘, ë¹¨ê°„ ì„¸ëª¨, ì´ˆë¡ ì› íƒì§€ ì „ìš©
"""

import os
import torch
import cv2
import numpy as np
import time
from PIL import Image
import transformers

Owlv2Processor = transformers.Owlv2Processor
Owlv2ForObjectDetection = transformers.Owlv2ForObjectDetection


class OWLv2VideoDetector:
    def __init__(self, model_id="google/owlv2-base-patch16-ensemble", confidence_threshold=0.1,
                 use_fp16=True, resize_input=True, frame_skip=1, input_size=(480, 320)):
        """OWLv2 ë¹„ë””ì˜¤ íƒì§€ê¸° ì´ˆê¸°í™”

        Args:
            model_id: ëª¨ë¸ ID
            confidence_threshold: ì‹ ë¢°ë„ ì„ê³„ê°’
            use_fp16: FP16 ë°˜ì •ë°€ë„ ì‚¬ìš© (ì†ë„ 2ë°° í–¥ìƒ)
            resize_input: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° ì¤„ì´ê¸° (ì†ë„ í–¥ìƒ)
            frame_skip: N í”„ë ˆì„ë§ˆë‹¤ íƒì§€ (1=ëª¨ë“  í”„ë ˆì„, 2=ê²©í”„ë ˆì„)
            input_size: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (width, height) - ì‘ì„ìˆ˜ë¡ ë¹ ë¦„
        """
        print(f"=" * 80)
        print(f"OWLv2 ë¹„ë””ì˜¤ ì‹¤ì‹œê°„ íƒì§€ ì´ˆê¸°í™” (ìµœì í™” ëª¨ë“œ)")
        print(f"=" * 80)

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.threshold = confidence_threshold
        self.use_fp16 = use_fp16 and self.device == "cuda"
        self.resize_input = resize_input
        self.frame_skip = frame_skip
        self.resize_size = input_size if resize_input else None

        # ëª¨ë¸ ë¡œë“œ
        print(f"ëª¨ë¸ ë¡œë”©: {model_id}")
        self.processor = Owlv2Processor.from_pretrained(model_id)
        self.model = Owlv2ForObjectDetection.from_pretrained(model_id)
        self.model.to(self.device)

        # FP16 ìµœì í™”
        if self.use_fp16:
            self.model.half()
            print("âœ“ FP16 ë°˜ì •ë°€ë„ í™œì„±í™” (ì†ë„ 2ë°° í–¥ìƒ)")

        # í‰ê°€ ëª¨ë“œ (dropout/batchnorm ë¹„í™œì„±í™”)
        self.model.eval()

        # CUDA ìµœì í™”
        if self.device == "cuda":
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            print("âœ“ CUDA ìµœì í™” í™œì„±í™” (cuDNN benchmark, TF32)")

        # PyTorch 2.0+ compile (OWLv2ì™€ í˜¸í™˜ ë¬¸ì œë¡œ ë¹„í™œì„±í™”)
        # try:
        #     if hasattr(torch, 'compile') and self.device == "cuda":
        #         self.model = torch.compile(self.model, mode="reduce-overhead")
        #         print("âœ“ torch.compile() í™œì„±í™” (ì¶”ê°€ ì†ë„ í–¥ìƒ)")
        # except Exception as e:
        #     print(f"âš  torch.compile() ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")

        # íƒì§€ ëŒ€ìƒ: íŒŒë€ +, ë¹¨ê°„ ì„¸ëª¨, ì´ˆë¡ ì›
        self.target_queries = [
            "blue cross",
            "red triangle",
            "green circle"
        ]

        print(f"âœ“ íƒì§€ ëŒ€ìƒ: {self.target_queries}")
        if self.resize_input:
            print(f"âœ“ ì…ë ¥ ë¦¬ì‚¬ì´ì¦ˆ: {self.resize_size} (ì†ë„ í–¥ìƒ)")
        if self.frame_skip > 1:
            print(f"âœ“ í”„ë ˆì„ ìŠ¤í‚µ: {self.frame_skip} (íƒì§€ëŠ” {self.frame_skip}í”„ë ˆì„ë§ˆë‹¤)")
        print(f"âœ“ ì´ˆê¸°í™” ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")
        print("=" * 80)

    def detect_frame(self, frame_rgb, original_size=None):
        """í”„ë ˆì„ì—ì„œ ê°ì²´ íƒì§€ (RGB ì…ë ¥)"""
        # NumPy -> PIL
        image = Image.fromarray(frame_rgb)
        original_size = original_size or image.size

        # ì…ë ¥ ë¦¬ì‚¬ì´ì¦ˆ (ì†ë„ í–¥ìƒ)
        if self.resize_input and self.resize_size:
            image = image.resize(self.resize_size, Image.BILINEAR)

        # ì…ë ¥ ì¤€ë¹„
        inputs = self.processor(
            text=self.target_queries,
            images=image,
            return_tensors="pt"
        )

        # NumPy -> Torch í…ì„œ
        inputs = {
            k: torch.from_numpy(v) if isinstance(v, np.ndarray) else v
            for k, v in inputs.items()
        }
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # FP16 ë³€í™˜
        if self.use_fp16:
            inputs = {k: v.half() if v.dtype == torch.float32 else v for k, v in inputs.items()}

        # íƒì§€ (autocastë¡œ ì¶”ê°€ ìµœì í™”)
        with torch.no_grad(), torch.amp.autocast('cuda', enabled=self.use_fp16):
            outputs = self.model(**inputs)

        # í›„ì²˜ë¦¬ (ì›ë³¸ í¬ê¸°ë¡œ ìŠ¤ì¼€ì¼ë§)
        target_sizes = torch.tensor([original_size[::-1]]).to(self.device)  # [H, W]
        results = self.processor.post_process_object_detection(
            outputs=outputs,
            threshold=self.threshold,
            target_sizes=target_sizes
        )[0]

        # ê²°ê³¼ íŒŒì‹±
        detections = []
        for box, score, label_idx in zip(results["boxes"], results["scores"], results["labels"]):
            box = box.cpu().tolist()
            score = score.cpu().item()
            label = self.target_queries[label_idx.cpu().item()]

            detections.append({
                "label": label,
                "confidence": score,
                "bbox": [int(b) for b in box]  # [x1, y1, x2, y2]
            })

        return detections

    def process_video(self, video_path, output_path, display=True):
        """ë¹„ë””ì˜¤ ì‹¤ì‹œê°„ íƒì§€"""
        print(f"\në¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘: {video_path}")

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"ì˜¤ë¥˜: ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨")
            return None

        # ë¹„ë””ì˜¤ ì†ì„±
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        print(f"í•´ìƒë„: {width}x{height}, FPS: {fps}, ì´ í”„ë ˆì„: {total_frames}")

        # ì¶œë ¥ ë¹„ë””ì˜¤
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        # ë””ìŠ¤í”Œë ˆì´ ì°½ ì´ˆê¸°í™”
        if display:
            cv2.namedWindow('OWLv2 Detection', cv2.WINDOW_NORMAL)
            cv2.resizeWindow('OWLv2 Detection', 1280, 720)

        frame_count = 0
        total_time = 0
        last_detections = []  # ë§ˆì§€ë§‰ íƒì§€ ê²°ê³¼ ìºì‹œ (í”„ë ˆì„ ìŠ¤í‚µìš©)

        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                frame_count += 1
                start_time = time.time()

                # í”„ë ˆì„ ìŠ¤í‚µ ë¡œì§: N í”„ë ˆì„ë§ˆë‹¤ë§Œ íƒì§€
                if frame_count % self.frame_skip == 1:
                    # BGR -> RGB
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                    # OWLv2 íƒì§€ (ì›ë³¸ í¬ê¸° ì •ë³´ ì „ë‹¬)
                    detections = self.detect_frame(frame_rgb, original_size=(width, height))
                    last_detections = detections  # ê²°ê³¼ ìºì‹±

                    inference_time = time.time() - start_time
                    total_time += inference_time
                else:
                    # ìŠ¤í‚µëœ í”„ë ˆì„ì€ ë§ˆì§€ë§‰ íƒì§€ ê²°ê³¼ ì¬ì‚¬ìš©
                    detections = last_detections
                    inference_time = 0  # íƒì§€ ì•ˆ í•¨

                # ì‹œê°í™”
                frame = self._draw(frame, detections, inference_time, frame_count, total_frames)
                out.write(frame)

                if display:
                    cv2.imshow('OWLv2 Detection', frame)
                    # ì ì ˆí•œ ë”œë ˆì´ (30fps ê¸°ì¤€ ì•½ 33ms)
                    key = cv2.waitKey(max(1, int(1000/fps))) & 0xFF
                    if key == ord('q'):
                        print("\nì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                        break

                # ì§„í–‰ ìƒí™©
                if frame_count % max(1, total_frames // 10) == 0:
                    progress = (frame_count / total_frames) * 100
                    actual_fps = frame_count / total_time if total_time > 0 else 0
                    print(f"ì§„í–‰: {progress:.0f}% | ì‹¤ì œ FPS: {actual_fps:.1f} | íƒì§€: {len(detections)}ê°œ")

        finally:
            cap.release()
            out.release()
            if display:
                cv2.destroyAllWindows()

        avg_fps = frame_count / total_time if total_time > 0 else 0
        print(f"\nì™„ë£Œ! í‰ê·  FPS: {avg_fps:.1f}")
        print(f"ì €ì¥: {output_path}\n")

        return {"frames": frame_count, "avg_fps": avg_fps}

    def _draw(self, frame, detections, inference_time, frame_num, total_frames):
        """íƒì§€ ê²°ê³¼ ê·¸ë¦¬ê¸°"""
        h, w = frame.shape[:2]

        # ìƒ‰ìƒ ë§¤í•‘
        colors = {
            "blue": (255, 0, 0),
            "red": (0, 0, 255),
            "green": (0, 255, 0)
        }

        # íƒì§€ ë°•ìŠ¤
        for det in detections:
            x1, y1, x2, y2 = det["bbox"]
            label = det["label"]
            conf = det["confidence"]

            # ìƒ‰ìƒ ì„ íƒ
            color = (255, 255, 255)
            for c_name, c_val in colors.items():
                if c_name in label.lower():
                    color = c_val
                    break

            # ë°•ìŠ¤
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)

            # ë¼ë²¨
            text = f"{label}: {conf:.2f}"
            cv2.putText(frame, text, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

        # ì •ë³´ í‘œì‹œ
        if inference_time > 0:
            info = f"OWLv2 | {inference_time*1000:.0f}ms | FPS: {1/inference_time:.1f} | Frame: {frame_num}/{total_frames}"
        else:
            info = f"OWLv2 | CACHED | Frame: {frame_num}/{total_frames}"
        cv2.rectangle(frame, (5, 5), (w-5, 40), (0, 0, 0), -1)
        cv2.putText(frame, info, (10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

        return frame


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    test_video_dir = "/home/yune/Kaboat2025_sim/test_video"
    test_vla_dir = "/home/yune/Kaboat2025_sim/test_vla"

    print("\n" + "=" * 80)
    print("OWLv2 ë¹„ë””ì˜¤ ì‹¤ì‹œê°„ íƒì§€ (ê·¹í•œ ìµœì í™” ëª¨ë“œ)")
    print("íƒì§€ ëŒ€ìƒ: íŒŒë€ +, ë¹¨ê°„ ì„¸ëª¨, ì´ˆë¡ ì›")
    print("=" * 80 + "\n")

    # íƒì§€ê¸° ì´ˆê¸°í™” (ê·¹í•œ ìµœì í™”)
    detector = OWLv2VideoDetector(
        model_id="google/owlv2-base-patch16-ensemble",
        confidence_threshold=0.3,
        use_fp16=True,        # FP16 ë°˜ì •ë°€ë„ (ì†ë„ 2ë°°)
        resize_input=True,    # ì…ë ¥ ë¦¬ì‚¬ì´ì¦ˆ (ì†ë„ í–¥ìƒ)
        frame_skip=10,         # 5í”„ë ˆì„ë§ˆë‹¤ íƒì§€ (ì†ë„ 5ë°°)
        input_size=(320, 240) # ë§¤ìš° ì‘ì€ ì…ë ¥ (ì†ë„ 4ë°°)
    )

    print("\nğŸ’¡ ì†ë„ ìš°ì„  ì„¤ì •:")
    print("  - ì…ë ¥ í¬ê¸°: 320Ã—240 (ì›ë³¸ì˜ 1/4)")
    print("  - í”„ë ˆì„ ìŠ¤í‚µ: 5 (5í”„ë ˆì„ë§ˆë‹¤ íƒì§€)")
    print("  - ì˜ˆìƒ ì†ë„ í–¥ìƒ: 10-20ë°°\n")

    # ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
    video_exts = {'.mp4', '.avi', '.mov', '.mkv'}
    video_files = [
        f for f in os.listdir(test_video_dir)
        if os.path.splitext(f.lower())[1] in video_exts
    ] if os.path.exists(test_video_dir) else []

    if not video_files:
        print(f"ë¹„ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {test_video_dir}")
        return

    # ê° ë¹„ë””ì˜¤ ì²˜ë¦¬
    for video_file in video_files:
        video_path = os.path.join(test_video_dir, video_file)
        output_name = f"owlv2_{os.path.splitext(video_file)[0]}.mp4"
        output_path = os.path.join(test_vla_dir, output_name)

        # display=True: ì‹¤ì‹œê°„ í™”ë©´ í‘œì‹œ (qí‚¤ë¡œ ì¢…ë£Œ)
        # display=False: í™”ë©´ ì—†ì´ ë¹„ë””ì˜¤ë§Œ ì €ì¥
        detector.process_video(video_path, output_path, display=True)

    print("=" * 80)
    print(f"âœ“ ì „ì²´ ì™„ë£Œ: {len(video_files)}ê°œ ë¹„ë””ì˜¤")
    print("=" * 80)


if __name__ == "__main__":
    main()
