"""
YOLOE ë¹„ë””ì˜¤ ì‹¤ì‹œê°„ íƒì§€
íŒŒë€ +ëª¨ì–‘, ë¹¨ê°„ ì„¸ëª¨, ì´ˆë¡ ì› íƒì§€ ì „ìš©
"""

import os
import torch
import cv2
import time
from ultralytics import YOLO


class YOLOEVideoDetector:
    def __init__(self, model_name="yoloe-v8l-seg.pt", confidence_threshold=0.25):
        """YOLOE ë¹„ë””ì˜¤ íƒì§€ê¸° ì´ˆê¸°í™”"""
        print(f"=" * 80)
        print(f"YOLOE ë¹„ë””ì˜¤ ì‹¤ì‹œê°„ íƒì§€ ì´ˆê¸°í™”")
        print(f"=" * 80)

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.threshold = confidence_threshold

        # ëª¨ë¸ ë¡œë“œ
        print(f"ëª¨ë¸ ë¡œë”©: {model_name}")
        self.model = YOLO(model_name)
        self.model.to(self.device)

        # íƒì§€ ëŒ€ìƒ: êµ¬ì²´ì ì¸ ì„¤ëª… ì‚¬ìš© (ê°œì„ )
        self.target_classes = [
            # íŒŒë€ ì‹­ìê°€/í”ŒëŸ¬ìŠ¤
            "blue cross marker", "blue plus sign", "blue cross shape",
            "blue x marker", "blue plus shape",

            # ë¹¨ê°„ ì‚¼ê°í˜•
            "red triangle marker", "red triangular shape", "red triangle sign",
            "red triangular marker",

            # ì´ˆë¡ ì›
            "green circle marker", "green circular shape", "green round marker",
            "green circular marker", "green round shape"
        ]

        # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        if hasattr(self.model, 'set_classes'):
            self.model.set_classes(self.target_classes)
            print(f"âœ“ íƒì§€ ëŒ€ìƒ: {len(self.target_classes)}ê°œ êµ¬ì²´ì  í´ë˜ìŠ¤")

        print(f"âœ“ ì´ˆê¸°í™” ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")
        print("=" * 80)

    def process_video(self, video_path, output_path, display=True):
        """ë¹„ë””ì˜¤ ì‹¤ì‹œê°„ íƒì§€"""
        print(f"\në¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘: {video_path}")

        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"ì˜¤ë¥˜: ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨")
            return None

        # ë¹„ë””ì˜¤ ì†ì„±
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        print(f"í•´ìƒë„: {width}x{height}, FPS: {fps}, ì´ í”„ë ˆì„: {total_frames}")

        # ì¶œë ¥ ë¹„ë””ì˜¤
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        # ë””ìŠ¤í”Œë ˆì´ ì°½ ì´ˆê¸°í™”
        if display:
            cv2.namedWindow('YOLOE Detection', cv2.WINDOW_NORMAL)
            cv2.resizeWindow('YOLOE Detection', 1280, 720)

        frame_count = 0
        total_time = 0

        try:
            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                frame_count += 1
                start_time = time.time()

                # BGR -> RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # YOLOE íƒì§€
                results = self.model.predict(
                    source=frame_rgb,
                    conf=self.threshold,
                    device=self.device,
                    verbose=False,
                    imgsz=640
                )

                inference_time = time.time() - start_time
                total_time += inference_time

                # íƒì§€ ê²°ê³¼ íŒŒì‹±
                detections = []
                if len(results) > 0 and results[0].boxes is not None:
                    boxes = results[0].boxes
                    for i in range(len(boxes)):
                        bbox = boxes.xyxy[i].cpu().numpy()
                        conf = float(boxes.conf[i].cpu().numpy())
                        cls_id = int(boxes.cls[i].cpu().numpy())

                        if conf >= self.threshold and cls_id < len(self.target_classes):
                            label = self.target_classes[cls_id]
                            x1, y1, x2, y2 = bbox
                            detections.append({
                                "label": label,
                                "confidence": conf,
                                "bbox": [int(x1), int(y1), int(x2), int(y2)]
                            })

                # ì‹œê°í™”
                frame = self._draw(frame, detections, inference_time, frame_count, total_frames)
                out.write(frame)

                if display:
                    cv2.imshow('YOLOE Detection', frame)
                    # ì ì ˆí•œ ë”œë ˆì´ (30fps ê¸°ì¤€ ì•½ 33ms)
                    key = cv2.waitKey(max(1, int(1000/fps))) & 0xFF
                    if key == ord('q'):
                        print("\nì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
                        break

                # ì§„í–‰ ìƒí™©
                if frame_count % max(1, total_frames // 10) == 0:
                    progress = (frame_count / total_frames) * 100
                    avg_fps = frame_count / total_time
                    print(f"ì§„í–‰: {progress:.0f}% | FPS: {avg_fps:.1f} | íƒì§€: {len(detections)}ê°œ")

        finally:
            cap.release()
            out.release()
            if display:
                cv2.destroyAllWindows()

        avg_fps = frame_count / total_time if total_time > 0 else 0
        print(f"\nì™„ë£Œ! í‰ê·  FPS: {avg_fps:.1f}")
        print(f"ì €ì¥: {output_path}\n")

        return {"frames": frame_count, "avg_fps": avg_fps}

    def _draw(self, frame, detections, inference_time, frame_num, total_frames):
        """íƒì§€ ê²°ê³¼ ê·¸ë¦¬ê¸°"""
        h, w = frame.shape[:2]

        # ìƒ‰ìƒ ë§¤í•‘
        colors = {
            "blue": (255, 0, 0),
            "red": (0, 0, 255),
            "green": (0, 255, 0)
        }

        # íƒì§€ ë°•ìŠ¤
        for det in detections:
            x1, y1, x2, y2 = det["bbox"]
            label = det["label"]
            conf = det["confidence"]

            # ìƒ‰ìƒ ì„ íƒ
            color = (255, 255, 255)
            for c_name, c_val in colors.items():
                if c_name in label.lower():
                    color = c_val
                    break

            # ë°•ìŠ¤
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)

            # ë¼ë²¨
            text = f"{label}: {conf:.2f}"
            cv2.putText(frame, text, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

        # ì •ë³´ í‘œì‹œ
        info = f"YOLOE | {inference_time*1000:.0f}ms | FPS: {1/inference_time:.1f} | Frame: {frame_num}/{total_frames}"
        cv2.rectangle(frame, (5, 5), (w-5, 40), (0, 0, 0), -1)
        cv2.putText(frame, info, (10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

        return frame


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    test_video_dir = "/home/yune/Kaboat2025_sim/test_video"
    test_vla_dir = "/home/yune/Kaboat2025_sim/test_vla"

    print("\n" + "=" * 80)
    print("YOLOE ë¹„ë””ì˜¤ ì‹¤ì‹œê°„ íƒì§€")
    print("íƒì§€ ëŒ€ìƒ: íŒŒë€ +, ë¹¨ê°„ ì„¸ëª¨, ì´ˆë¡ ì›")
    print("=" * 80 + "\n")

    # íƒì§€ê¸° ì´ˆê¸°í™”
    detector = YOLOEVideoDetector(
        model_name="yoloe-v8l-seg.pt",
        confidence_threshold=0.15
    )

    print("ğŸ’¡ ê°œì„ : êµ¬ì²´ì ì¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš© (ì •í™•ë„ í–¥ìƒ)\n")

    # ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
    video_exts = {'.mp4', '.avi', '.mov', '.mkv'}
    video_files = [
        f for f in os.listdir(test_video_dir)
        if os.path.splitext(f.lower())[1] in video_exts
    ] if os.path.exists(test_video_dir) else []

    if not video_files:
        print(f"ë¹„ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {test_video_dir}")
        return

    # ê° ë¹„ë””ì˜¤ ì²˜ë¦¬
    for video_file in video_files:
        video_path = os.path.join(test_video_dir, video_file)
        output_name = f"yoloe_{os.path.splitext(video_file)[0]}.mp4"
        output_path = os.path.join(test_vla_dir, output_name)

        # display=True: ì‹¤ì‹œê°„ í™”ë©´ í‘œì‹œ (qí‚¤ë¡œ ì¢…ë£Œ)
        # display=False: í™”ë©´ ì—†ì´ ë¹„ë””ì˜¤ë§Œ ì €ì¥
        detector.process_video(video_path, output_path, display=True)

    print("=" * 80)
    print(f"âœ“ ì „ì²´ ì™„ë£Œ: {len(video_files)}ê°œ ë¹„ë””ì˜¤")
    print("=" * 80)


if __name__ == "__main__":
    main()
